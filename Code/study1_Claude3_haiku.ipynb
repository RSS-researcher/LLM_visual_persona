{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files from 1 to 500 (total 500 files).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  23%|██▎       | 116/500 [28:15<1:38:44, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during send_message attempt 1: An error occurred (ValidationException) when calling the InvokeModel operation: Could not process image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  38%|███▊      | 189/500 [46:56<1:17:49, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during send_message attempt 1: An error occurred (ValidationException) when calling the InvokeModel operation: Could not process image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 500/500 [2:08:29<00:00, 15.42s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to C:/Users/dssalpc/Desktop/aai/dataset_5_14\\study1_haiku_round1_1_500.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import re\n",
    "from botocore.config import Config\n",
    "\n",
    "# Config with increased timeout for Boto3 client\n",
    "config = Config(read_timeout=300)\n",
    "\n",
    "# OpenAI API Key (replace with your own API key)\n",
    "my_api_key = \"your_openai_api_key\"\n",
    "client = OpenAI(api_key=my_api_key, timeout=20.0)\n",
    "\n",
    "# Initialize the Boto3 client for AWS Bedrock\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime', config=config)\n",
    "\n",
    "class DialogModel:\n",
    "    def __init__(self, engine=\"anthropic.claude-3-haiku-20240307-v1:0\", initial_prompt=\"\"):\n",
    "        \"\"\"\n",
    "        Initialize the DialogModel class.\n",
    "        :param engine: Name of the GPT engine to use\n",
    "        \"\"\"\n",
    "        self.engine = engine\n",
    "        self.messages = []\n",
    "        self.initial_system_prompt = initial_prompt\n",
    "        self.system_prompt = initial_prompt\n",
    "\n",
    "    def add_role_prompt(self, round_number):\n",
    "        if round_number % 2 != 0:  # Rounds 1, 3 (proposer)\n",
    "            return \"Now you are the proposer in this round. Propose a division ratio.\"\n",
    "        else:  # Rounds 2, 4 (responder)\n",
    "            return \"Now you are the responder in this round. As the responder, never make a counter-proposal; only accept or reject the proposal.\"\n",
    "    \n",
    "    def set_prompts(self, system_prompt, user_prompt):\n",
    "        \"\"\"\n",
    "        Set the system prompt and user prompt and initialize the message array.\n",
    "        :param system_prompt: Common system prompt\n",
    "        :param user_prompt: User prompt specific to the model\n",
    "        \"\"\"\n",
    "        self.initial_system_prompt = system_prompt  # Update initial system prompt\n",
    "        self.system_prompt = system_prompt\n",
    "        self.messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        \n",
    "    def send_message(self, message=None, round_number=None):\n",
    "        if message:\n",
    "            last_message = self.messages[-1]\n",
    "            if last_message[\"role\"] == \"user\":\n",
    "                last_message[\"content\"][-1][\"text\"] += f\"\\n{message}\"  # Append message to the last text piece\n",
    "            else:\n",
    "                self.messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]})\n",
    "\n",
    "        if round_number is not None:\n",
    "            # Add role prompt just before sending the message\n",
    "            role_prompt = self.add_role_prompt(round_number)\n",
    "            current_prompt = self.initial_system_prompt + \"\\n\" + role_prompt\n",
    "        else:\n",
    "            current_prompt = self.initial_system_prompt\n",
    "            \n",
    "        for attempt in range(5):  # Maximum 5 attempts\n",
    "            try:\n",
    "                body = json.dumps(\n",
    "                    {\n",
    "                        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                        \"system\": current_prompt,\n",
    "                        \"messages\": self.messages,\n",
    "                        \"max_tokens\": 300,\n",
    "                        \"temperature\": 1\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                response = bedrock_runtime.invoke_model(body=body, modelId=self.engine)\n",
    "                response_body = response['body'].read().decode('utf-8')\n",
    "                response_data = json.loads(response_body)\n",
    "                reply = response_data['content'][0]['text']\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during send_message attempt {attempt + 1}: {e}\")\n",
    "                if 'Connection error' in str(e):\n",
    "                    print(\"Connection error detected. Waiting for 5 minutes before retrying...\")\n",
    "                    time.sleep(300)  # Wait for 5 minutes\n",
    "                else:\n",
    "                    time.sleep(1)  # Wait for 1 second for other errors\n",
    "                if attempt == 4:\n",
    "                    print(f\"Failed to process message after 5 attempts. Skipping file.\")\n",
    "                    return None\n",
    "\n",
    "        last_message = self.messages[-1]\n",
    "        if last_message[\"role\"] == \"assistant\":\n",
    "            last_message[\"content\"] += f\"\\n{reply}\"\n",
    "        else:\n",
    "            self.messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply\n",
    "\n",
    "    def reset_dialog(self):\n",
    "        \"\"\"\n",
    "        Reset the dialog history.\n",
    "        \"\"\"\n",
    "        self.messages = []\n",
    "        \n",
    "    def update_message_with_reply(self, reply):\n",
    "        last_message = self.messages[-1]\n",
    "        if last_message[\"role\"] == \"user\":\n",
    "            if isinstance(last_message[\"content\"], list):\n",
    "                last_message[\"content\"].append({\"type\": \"text\", \"text\": reply})\n",
    "            else:\n",
    "                last_message[\"content\"] = [{\"type\": \"text\", \"text\": reply}]\n",
    "        else:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": reply}]})\n",
    "\n",
    "    \n",
    "    def add_round_marker(self, round_number):\n",
    "        round_marker = f\"===round{round_number}===\"\n",
    "        last_message = self.messages[-1]\n",
    "        if last_message[\"role\"] == \"user\":\n",
    "            if isinstance(last_message[\"content\"], list):\n",
    "                last_message[\"content\"].append({\"type\": \"text\", \"text\": round_marker})\n",
    "            else:\n",
    "                last_message[\"content\"] = [{\"type\": \"text\", \"text\": last_message[\"content\"] + \"\\n\" + round_marker}]\n",
    "        else:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": round_marker}]})\n",
    "\n",
    "\n",
    "def extract_proposal(dialogue):\n",
    "    for attempt in range(5):  # Maximum 5 attempts\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Based on the following dialogue, extract the amount the proposer proposed for him or her self against the responder. Answer with only one number. Do not use '$'.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Dialogue:\\n{dialogue}\"}\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during extract_proposal attempt {attempt + 1}: {e}\")\n",
    "            if 'Connection error' in str(e):\n",
    "                print(\"Connection error detected. Waiting for 5 minutes before retrying...\")\n",
    "                time.sleep(300)  # Wait for 5 minutes\n",
    "            else:\n",
    "                time.sleep(1)  # Wait for 1 second for other errors\n",
    "            if attempt == 4:\n",
    "                print(f\"Failed to extract proposal after 5 attempts. Skipping this step.\")\n",
    "                return None\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def extract_acceptance(dialogue):\n",
    "    for attempt in range(5):  # Maximum 5 attempts\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Based on the following dialogue, If the responder accepts the proposal, print '1', and if it rejects, print '0'. Answer with only one number.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Dialogue:\\n{dialogue}\"}\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during extract_acceptance attempt {attempt + 1}: {e}\")\n",
    "            if 'Connection error' in str(e):\n",
    "                print(\"Connection error detected. Waiting for 5 minutes before retrying...\")\n",
    "                time.sleep(300)  # Wait for 5 minutes\n",
    "            else:\n",
    "                time.sleep(1)  # Wait for 1 second for other errors\n",
    "            if attempt == 4:\n",
    "                print(f\"Failed to extract acceptance after 5 attempts. Skipping this step.\")\n",
    "                return None\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def automated_dialogue(model1, manual_responses, number_of_turns):\n",
    "    \"\"\"\n",
    "    Conducts a conversation between Model 1 and manual responses.\n",
    "    :param model1: Model to generate dialogue\n",
    "    :param manual_responses: Manual responses for the other model\n",
    "    :param number_of_turns: Number of dialogue turns\n",
    "    \"\"\"\n",
    "    dialogue_results = \"\"  # To store the entire dialogue\n",
    "    rounds_data = []  # To store data for each round\n",
    "    dialogue_data = []\n",
    "\n",
    "    for turn in range(number_of_turns):\n",
    "        model1.add_round_marker(turn + 1)\n",
    "        round_info = {'round': turn + 1, 'model1_role': '', 'model1_text': '', 'proposal_amount': '',\n",
    "                      'model2_text': '', 'proposal_acceptance': ''}\n",
    "\n",
    "        if turn % 2 == 0:\n",
    "            reply_from_model1 = model1.send_message(round_number=turn+1)\n",
    "            if reply_from_model1 is None:\n",
    "                continue\n",
    "            proposal_amount = extract_proposal(reply_from_model1)\n",
    "            if proposal_amount is None:\n",
    "                skipped_files.append(f\"{model1.engine}_round{turn+1}_proposal\")\n",
    "                continue\n",
    "            round_info['model1_role'] = 'Proposer'\n",
    "            round_info['model1_text'] = reply_from_model1\n",
    "            round_info['proposal_amount'] = proposal_amount\n",
    "            dialogue_results += f\"Round {turn+1}, Model 1 (Proposer): {reply_from_model1}\\n\"\n",
    "            model1.update_message_with_reply(manual_responses[turn])\n",
    "            proposal_acceptance = 1\n",
    "            if proposal_acceptance is None:\n",
    "                skipped_files.append(f\"{model1.engine}_round{turn+1}_acceptance\")\n",
    "                continue\n",
    "            round_info['model2_text'] = manual_responses[turn]\n",
    "            round_info['proposal_acceptance'] = proposal_acceptance\n",
    "            dialogue_results += f\"Round {turn+1}, Model 2 (Responder): {manual_responses[turn]}\\n\"\n",
    "            dialogue_data.extend([round_info['model1_text'], round_info['model2_text']])\n",
    "        else:\n",
    "            model1.update_message_with_reply(manual_responses[turn])\n",
    "            if turn == 1:\n",
    "                proposal_amount = 50\n",
    "            else:\n",
    "                proposal_amount = 75\n",
    "            \n",
    "            if proposal_amount is None:\n",
    "                skipped_files.append(f\"{model1.engine}_round{turn+1}_proposal\")\n",
    "                continue\n",
    "            round_info['model2_text'] = manual_responses[turn]\n",
    "            round_info['proposal_amount'] = proposal_amount\n",
    "            dialogue_results += f\"Round {turn+1}, Model 2 (Proposer): {manual_responses[turn]}\\n\"\n",
    "            reply_from_model1 = model1.send_message(round_number=turn+1)\n",
    "            if reply_from_model1 is None:\n",
    "                continue\n",
    "            proposal_acceptance = extract_acceptance(reply_from_model1)\n",
    "            if proposal_acceptance is None:\n",
    "                skipped_files.append(f\"{model1.engine}_round{turn+1}_acceptance\")\n",
    "                continue\n",
    "            round_info['model1_role'] = 'Responder'\n",
    "            round_info['model1_text'] = reply_from_model1\n",
    "            round_info['proposal_acceptance'] = proposal_acceptance\n",
    "            dialogue_results += f\"Round {turn+1}, Model 1 (Responder): {reply_from_model1}\\n\"\n",
    "            dialogue_data.extend([round_info['model2_text'], round_info['model1_text']])\n",
    "\n",
    "        rounds_data.append(round_info)\n",
    "        dialogue_data.extend([round_info['proposal_amount'], round_info['proposal_acceptance']])\n",
    "\n",
    "    columns = []\n",
    "    for i in range(number_of_turns):\n",
    "        columns.extend([\n",
    "            f\"text_{i*2+1}\", f\"text_{i*2+2}\",\n",
    "            f\"proposal_amount_round_{i+1}\", f\"proposal_acceptance_round_{i+1}\"\n",
    "        ])\n",
    "    df = pd.DataFrame([dialogue_data], columns=columns)\n",
    "    return dialogue_results, rounds_data, df\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def save_progress(df_all: pd.DataFrame, base_path: str, start_file_num: int, end_file_num: int):\n",
    "    \"\"\"\n",
    "    Save progress to an Excel file.\n",
    "    :param df_all: The complete DataFrame\n",
    "    :param base_path: Path to save the file\n",
    "    :param start_file_num: Starting file number\n",
    "    :param end_file_num: Ending file number\n",
    "    \"\"\"\n",
    "    columns_reordered = ['image_file']\n",
    "    number_of_turns = 4  # Assuming a fixed number of turns\n",
    "    columns_text = []\n",
    "    columns_proposals = []\n",
    "    columns_acceptance = []\n",
    "    for i in range(1, number_of_turns + 1):\n",
    "        columns_text.extend([f\"text_{i*2-1}\", f\"text_{i*2}\"])\n",
    "        columns_proposals.append(f\"proposal_amount_round_{i}\")\n",
    "        columns_acceptance.append(f\"proposal_acceptance_round_{i}\")\n",
    "\n",
    "    columns_reordered.extend(columns_text + columns_proposals + columns_acceptance)\n",
    "    df_all = df_all[columns_reordered]\n",
    "\n",
    "    # Save DataFrame to Excel file\n",
    "    excel_file_path = os.path.join(base_path, f'study1_haiku_round1_{start_file_num}_{end_file_num}.xlsx')\n",
    "    df_all.to_excel(excel_file_path, index=False)\n",
    "    print(f\"DataFrame saved to {excel_file_path}\")\n",
    "\n",
    "# Manual responses to be used in the dialogue\n",
    "manual_responses = [\n",
    "    \"As the responder, I accept your proposal.\",\n",
    "    \"As the proposer, I suggest an even split, $50 for me and $50 for you.\",\n",
    "    \"As the responder, I accept your proposal.\",\n",
    "    \"As the proposer, I suggest a division of $75 for me and $25 for you.\"\n",
    "]\n",
    "\n",
    "# Initial system prompt for the dialogue\n",
    "system_prompt1 = \"\"\"\n",
    "In the following conversation, you are one of the two players participating in a negotiation game.\n",
    "\"\"\"\n",
    "\n",
    "# User input for file processing range\n",
    "start_file_num = int(input(\"Enter the starting file number (numeric only): \"))\n",
    "num_files_to_process = int(input(\"Enter the number of files to process: \"))\n",
    "end_file_num = start_file_num + num_files_to_process - 1\n",
    "\n",
    "# Function to extract number for sorting files numerically\n",
    "def extract_number(file_name):\n",
    "    match = re.search(r'(\\d+)', file_name)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# Specify the base path where image files are located\n",
    "base_path = \"/path/to/your/folder\"\n",
    "files = sorted([file for file in os.listdir(base_path) if file.endswith('.png') and int(file.split('.')[0]) >= start_file_num], key=extract_number)[:num_files_to_process]\n",
    "df_all = pd.DataFrame()\n",
    "skipped_files = []\n",
    "print(f\"Processing files from {start_file_num} to {end_file_num} (total {num_files_to_process} files).\")\n",
    "\n",
    "try:\n",
    "    for file in tqdm(files, desc=\"Processing files\"):\n",
    "        if file.endswith('.png'):  # Process only image files\n",
    "            image_path = os.path.join(base_path, file)\n",
    "            base64_image = encode_image(image_path)\n",
    "            user_prompt_model1 = [\n",
    "                {\"type\": \"text\", \"text\": \"You are the character in the following image:\"},\n",
    "                {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": f\"{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"\"\"\n",
    "                 Consider your character's traits in upcoming conversations.\n",
    "                 \n",
    "                 Let's play an ultimatum game. The rules of the game are as follows: \n",
    "                 (1) Two players divide $100 in each round. \n",
    "                 (2) In each round, one player proposes a division ratio, and the other player can only accept or reject the proposal; one can not make a counter-proposal. \n",
    "                 (3) If the proposal is accepted, the money is divided according to the proposed ratio; if rejected, neither player gets any money. \n",
    "                 (4) The game consists of a total of four rounds, with players alternating roles between proposing and responding in each round.\n",
    "                 \"\"\"\n",
    "                 },\n",
    "                ]\n",
    "            model1 = DialogModel()\n",
    "            model1.set_prompts(system_prompt1, user_prompt_model1)\n",
    "            number_of_turns = 4\n",
    "            result, rounds_results, df = automated_dialogue(model1, manual_responses, number_of_turns)\n",
    "            if result:\n",
    "                df['image_file'] = file  # Add image file name to DataFrame\n",
    "                df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "            else:\n",
    "                skipped_files.append(file)\n",
    "            model1.reset_dialog()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    save_progress(df_all, base_path, start_file_num, end_file_num)\n",
    "    skipped_files.append(file)\n",
    "finally:\n",
    "    save_progress(df_all, base_path, start_file_num, end_file_num)\n",
    "    if skipped_files:\n",
    "        print(f\"Skipped files: {', '.join(skipped_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files from 1 to 22 (total 22 files).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 22/22 [04:23<00:00, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to C:/Users/dssalpc/Desktop/aai/dataset_5_14\\study1_haiku_round2_1_22.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import re\n",
    "\n",
    "from botocore.config import Config\n",
    "\n",
    "# Increase read timeout for boto3 client\n",
    "config = Config(read_timeout=300)\n",
    "\n",
    "# OpenAI API Key (replace with your own API key)\n",
    "my_api_key = \"your_openai_api_key\"\n",
    "client = OpenAI(api_key=my_api_key, timeout=20.0)\n",
    "\n",
    "# Initialize the Boto3 client for AWS Bedrock\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime', config=config)\n",
    "\n",
    "class DialogModel:\n",
    "    def __init__(self, engine=\"anthropic.claude-3-haiku-20240307-v1:0\", initial_prompt=\"\"):\n",
    "        \"\"\"\n",
    "        Initialize the DialogModel class.\n",
    "        :param engine: Name of the GPT engine to use\n",
    "        \"\"\"\n",
    "        self.engine = engine\n",
    "        self.messages = []\n",
    "        self.initial_system_prompt = initial_prompt\n",
    "        self.system_prompt = initial_prompt\n",
    "\n",
    "    def add_role_prompt(self, round_number):\n",
    "        if round_number % 2 != 0:  # Rounds 1, 3 (responder)\n",
    "            return \"Now you are the responder in this round. As the responder, never make a counter-proposal; only accept or reject the proposal.\"\n",
    "        else:  # Rounds 2, 4 (proposer)\n",
    "            return \"Now you are the proposer in this round. Propose a division ratio.\"\n",
    "    \n",
    "    def set_prompts(self, system_prompt, user_prompt):\n",
    "        \"\"\"\n",
    "        Set the system prompt and user prompt and initialize the message array.\n",
    "        :param system_prompt: Common system prompt\n",
    "        :param user_prompt: User prompt specific to the model\n",
    "        \"\"\"\n",
    "        self.initial_system_prompt = system_prompt  # Update initial system prompt\n",
    "        self.system_prompt = system_prompt\n",
    "        self.messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        \n",
    "    def send_message(self, message=None, round_number=None):\n",
    "        if message:\n",
    "            last_message = self.messages[-1]\n",
    "            if last_message[\"role\"] == \"user\":\n",
    "                last_message[\"content\"][-1][\"text\"] += f\"\\n{message}\"  # Append message to the last text piece\n",
    "            else:\n",
    "                self.messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]})\n",
    "\n",
    "        if round_number is not None:\n",
    "            # Add role prompt just before sending the message\n",
    "            role_prompt = self.add_role_prompt(round_number)\n",
    "            current_prompt = self.initial_system_prompt + \"\\n\" + role_prompt\n",
    "        else:\n",
    "            current_prompt = self.initial_system_prompt\n",
    "\n",
    "        for attempt in range(5):  # Maximum 5 attempts\n",
    "            try:\n",
    "                body = json.dumps(\n",
    "                    {\n",
    "                        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                        \"system\": current_prompt,\n",
    "                        \"messages\": self.messages,\n",
    "                        \"max_tokens\": 300,\n",
    "                        \"temperature\": 1\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                response = bedrock_runtime.invoke_model(body=body, modelId=self.engine)\n",
    "                response_body = response['body'].read().decode('utf-8')\n",
    "                response_data = json.loads(response_body)\n",
    "                reply = response_data['content'][0]['text']\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during send_message attempt {attempt + 1}: {e}\")\n",
    "                if 'Connection error' in str(e):\n",
    "                    print(\"Connection error detected. Waiting for 5 minutes before retrying...\")\n",
    "                    time.sleep(300)  # Wait for 5 minutes\n",
    "                else:\n",
    "                    time.sleep(1)  # Wait for 1 second for other errors\n",
    "                if attempt == 4:\n",
    "                    print(f\"Failed to process message after 5 attempts. Skipping file.\")\n",
    "                    return None\n",
    "\n",
    "        last_message = self.messages[-1]\n",
    "        if last_message[\"role\"] == \"assistant\":\n",
    "            last_message[\"content\"] += f\"\\n{reply}\"\n",
    "        else:\n",
    "            self.messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply\n",
    "\n",
    "    def reset_dialog(self):\n",
    "        \"\"\"\n",
    "        Reset the dialog history.\n",
    "        \"\"\"\n",
    "        self.messages = []\n",
    "        \n",
    "    def update_message_with_reply(self, reply):\n",
    "        last_message = self.messages[-1]\n",
    "        if last_message[\"role\"] == \"user\":\n",
    "            if isinstance(last_message[\"content\"], list):\n",
    "                last_message[\"content\"].append({\"type\": \"text\", \"text\": reply})\n",
    "            else:\n",
    "                last_message[\"content\"] = [{\"type\": \"text\", \"text\": reply}]\n",
    "        else:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": reply}]})\n",
    "\n",
    "    \n",
    "    def add_round_marker(self, round_number):\n",
    "        round_marker = f\"===round{round_number}===\"\n",
    "        last_message = self.messages[-1]\n",
    "        if last_message[\"role\"] == \"user\":\n",
    "            if isinstance(last_message[\"content\"], list):\n",
    "                last_message[\"content\"].append({\"type\": \"text\", \"text\": round_marker})\n",
    "            else:\n",
    "                last_message[\"content\"] = [{\"type\": \"text\", \"text\": last_message[\"content\"] + \"\\n\" + round_marker}]\n",
    "        else:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": round_marker}]})\n",
    "\n",
    "\n",
    "def extract_proposal(dialogue):\n",
    "    for attempt in range(5):  # Maximum 5 attempts\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Based on the following dialogue, extract the amount the proposer proposed for him or her self against the responder. Answer with only one number. Do not use '$'.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Dialogue:\\n{dialogue}\"}\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during extract_proposal attempt {attempt + 1}: {e}\")\n",
    "            if 'Connection error' in str(e):\n",
    "                print(\"Connection error detected. Waiting for 5 minutes before retrying...\")\n",
    "                time.sleep(300)  # Wait for 5 minutes\n",
    "            else:\n",
    "                time.sleep(1)  # Wait for 1 second for other errors\n",
    "            if attempt == 4:\n",
    "                print(f\"Failed to extract proposal after 5 attempts. Skipping this step.\")\n",
    "                return None\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def extract_acceptance(dialogue):\n",
    "    for attempt in range(5):  # Maximum 5 attempts\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Based on the following dialogue, If the responder accepts the proposal, print '1', and if it rejects, print '0'. Answer with only one number.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Dialogue:\\n{dialogue}\"}\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during extract_acceptance attempt {attempt + 1}: {e}\")\n",
    "            if 'Connection error' in str(e):\n",
    "                print(\"Connection error detected. Waiting for 5 minutes before retrying...\")\n",
    "                time.sleep(300)  # Wait for 5 minutes\n",
    "            else:\n",
    "                time.sleep(1)  # Wait for 1 second for other errors\n",
    "            if attempt == 4:\n",
    "                print(f\"Failed to extract acceptance after 5 attempts. Skipping this step.\")\n",
    "                return None\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def automated_dialogue(model1, manual_responses, number_of_turns):\n",
    "    \"\"\"\n",
    "    Conducts a conversation between Model 1 and manual responses.\n",
    "    :param model1: Model to generate dialogue\n",
    "    :param manual_responses: Manual responses for the other model\n",
    "    :param number_of_turns: Number of dialogue turns\n",
    "    \"\"\"\n",
    "    dialogue_results = \"\"  # To store the entire dialogue\n",
    "    rounds_data = []  # To store data for each round\n",
    "    dialogue_data = []\n",
    "\n",
    "    for turn in range(number_of_turns):\n",
    "        model1.add_round_marker(turn + 1)\n",
    "        round_info = {'round': turn + 1, 'model1_role': '', 'model1_text': '', 'proposal_amount': '',\n",
    "                      'model2_text': '', 'proposal_acceptance': ''}\n",
    "\n",
    "        if turn % 2 != 0:\n",
    "            reply_from_model1 = model1.send_message(round_number=turn+1)\n",
    "            if reply_from_model1 is None:\n",
    "                continue\n",
    "            proposal_amount = extract_proposal(reply_from_model1)\n",
    "            if proposal_amount is None:\n",
    "                skipped_files.append(f\"{model1.engine}_round{turn+1}_proposal\")\n",
    "                continue\n",
    "            round_info['model1_role'] = 'Proposer'\n",
    "            round_info['model1_text'] = reply_from_model1\n",
    "            round_info['proposal_amount'] = proposal_amount\n",
    "            dialogue_results += f\"Round {turn+1}, Model 1 (Proposer): {reply_from_model1}\\n\"\n",
    "            model1.update_message_with_reply(manual_responses[turn])\n",
    "            proposal_acceptance = 1\n",
    "            if proposal_acceptance is None:\n",
    "                skipped_files.append(f\"{model1.engine}_round{turn+1}_acceptance\")\n",
    "                continue\n",
    "            round_info['model2_text'] = manual_responses[turn]\n",
    "            round_info['proposal_acceptance'] = proposal_acceptance\n",
    "            dialogue_results += f\"Round {turn+1}, Model 2 (Responder): {manual_responses[turn]}\\n\"\n",
    "            dialogue_data.extend([round_info['model1_text'], round_info['model2_text']])\n",
    "        else:\n",
    "            model1.update_message_with_reply(manual_responses[turn])\n",
    "            if turn == 0:\n",
    "                proposal_amount = 50\n",
    "            else:\n",
    "                proposal_amount = 75\n",
    "            if proposal_amount is None:\n",
    "                skipped_files.append(f\"{model1.engine}_round{turn+1}_proposal\")\n",
    "                continue\n",
    "            round_info['model2_text'] = manual_responses[turn]\n",
    "            round_info['proposal_amount'] = proposal_amount\n",
    "            dialogue_results += f\"Round {turn+1}, Model 2 (Proposer): {manual_responses[turn]}\\n\"\n",
    "            reply_from_model1 = model1.send_message(round_number=turn+1)\n",
    "            if reply_from_model1 is None:\n",
    "                continue\n",
    "            proposal_acceptance = extract_acceptance(reply_from_model1)\n",
    "            if proposal_acceptance is None:\n",
    "                skipped_files.append(f\"{model1.engine}_round{turn+1}_acceptance\")\n",
    "                continue\n",
    "            round_info['model1_role'] = 'Responder'\n",
    "            round_info['model1_text'] = reply_from_model1\n",
    "            round_info['proposal_acceptance'] = proposal_acceptance\n",
    "            dialogue_results += f\"Round {turn+1}, Model 1 (Responder): {reply_from_model1}\\n\"\n",
    "            dialogue_data.extend([round_info['model2_text'], round_info['model1_text']])\n",
    "\n",
    "        rounds_data.append(round_info)\n",
    "        dialogue_data.extend([round_info['proposal_amount'], round_info['proposal_acceptance']])\n",
    "\n",
    "    columns = []\n",
    "    for i in range(number_of_turns):\n",
    "        columns.extend([\n",
    "            f\"text_{i*2+1}\", f\"text_{i*2+2}\",\n",
    "            f\"proposal_amount_round_{i+1}\", f\"proposal_acceptance_round_{i+1}\"\n",
    "        ])\n",
    "    df = pd.DataFrame([dialogue_data], columns=columns)\n",
    "    return dialogue_results, rounds_data, df\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def save_progress(df_all: pd.DataFrame, base_path: str, start_file_num: int, end_file_num: int):\n",
    "    \"\"\"\n",
    "    Save progress to an Excel file.\n",
    "    :param df_all: The complete DataFrame\n",
    "    :param base_path: Path to save the file\n",
    "    :param start_file_num: Starting file number\n",
    "    :param end_file_num: Ending file number\n",
    "    \"\"\"\n",
    "    columns_reordered = ['image_file']\n",
    "    number_of_turns = 4  # Assuming a fixed number of turns\n",
    "    columns_text = []\n",
    "    columns_proposals = []\n",
    "    columns_acceptance = []\n",
    "    for i in range(1, number_of_turns + 1):\n",
    "        columns_text.extend([f\"text_{i*2-1}\", f\"text_{i*2}\"])\n",
    "        columns_proposals.append(f\"proposal_amount_round_{i}\")\n",
    "        columns_acceptance.append(f\"proposal_acceptance_round_{i}\")\n",
    "\n",
    "    columns_reordered.extend(columns_text + columns_proposals + columns_acceptance)\n",
    "    df_all = df_all[columns_reordered]\n",
    "\n",
    "    # Save DataFrame to Excel file\n",
    "    excel_file_path = os.path.join(base_path, f'study1_haiku_round2_{start_file_num}_{end_file_num}.xlsx')\n",
    "    df_all.to_excel(excel_file_path, index=False)\n",
    "    print(f\"DataFrame saved to {excel_file_path}\")\n",
    "\n",
    "# Manual responses to be used in the dialogue\n",
    "manual_responses = [\n",
    "    \"As the proposer, I suggest an even split, $50 for me and $50 for you.\",\n",
    "    \"As the responder, I accept your proposal.\",\n",
    "    \"As the proposer, I suggest a division of $75 for me and $25 for you.\",\n",
    "    \"As the responder, I accept your proposal.\",\n",
    "]\n",
    "\n",
    "# Initial system prompt for the dialogue\n",
    "system_prompt1 = \"\"\"\n",
    "In the following conversation, you are one of the two players participating in a negotiation game.\n",
    "\"\"\"\n",
    "\n",
    "# User input for file processing range\n",
    "start_file_num = int(input(\"Enter the starting file number (numeric only): \"))\n",
    "num_files_to_process = int(input(\"Enter the number of files to process: \"))\n",
    "end_file_num = start_file_num + num_files_to_process - 1\n",
    "\n",
    "# Function to extract number for sorting files numerically\n",
    "def extract_number(file_name):\n",
    "    match = re.search(r'(\\d+)', file_name)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# Specify the base path where image files are located\n",
    "base_path = \"C:/Users/dssalpc/Desktop/aai/dataset_5_14\"\n",
    "files = sorted([file for file in os.listdir(base_path) if file.endswith('.png') and int(file.split('.')[0]) >= start_file_num], key=extract_number)[:num_files_to_process]\n",
    "df_all = pd.DataFrame()\n",
    "skipped_files = []\n",
    "print(f\"Processing files from {start_file_num} to {end_file_num} (total {num_files_to_process} files).\")\n",
    "\n",
    "try:\n",
    "    for file in tqdm(files, desc=\"Processing files\"):\n",
    "        if file.endswith('.png'):  # Process only image files\n",
    "            image_path = os.path.join(base_path, file)\n",
    "            base64_image = encode_image(image_path)\n",
    "            user_prompt_model1 = [\n",
    "                {\"type\": \"text\", \"text\": \"You are the character in the following image:\"},\n",
    "                {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": f\"{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"\"\"\n",
    "                 Consider your character's traits in upcoming conversations.\n",
    "                 \n",
    "                 Let's play an ultimatum game. The rules of the game are as follows: \n",
    "                 (1) Two players divide $100 in each round. \n",
    "                 (2) In each round, one player proposes a division ratio, and the other player can only accept or reject the proposal; one can not make a counter-proposal. \n",
    "                 (3) If the proposal is accepted, the money is divided according to the proposed ratio; if rejected, neither player gets any money. \n",
    "                 (4) The game consists of a total of four rounds, with players alternating roles between proposing and responding in each round.\n",
    "                 \"\"\"\n",
    "                 },\n",
    "                ]\n",
    "            model1 = DialogModel()\n",
    "            model1.set_prompts(system_prompt1, user_prompt_model1)\n",
    "            number_of_turns = 4\n",
    "            result, rounds_results, df = automated_dialogue(model1, manual_responses, number_of_turns)\n",
    "            if result:\n",
    "                df['image_file'] = file  # Add image file name to DataFrame\n",
    "                df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "            else:\n",
    "                skipped_files.append(file)\n",
    "            model1.reset_dialog()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    save_progress(df_all, base_path, start_file_num, end_file_num)\n",
    "    skipped_files.append(file)\n",
    "finally:\n",
    "    save_progress(df_all, base_path, start_file_num, end_file_num)\n",
    "    if skipped_files:\n",
    "        print(f\"Skipped files: {', '.join(skipped_files)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
